{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcN5XDlJuwWM"
   },
   "source": [
    "# 1. Install all requirements listed\n",
    "## Create a conda enviroment with python version 3.9 and install all packages below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "72stdNLhuwWQ",
    "outputId": "13f98c57-3d40-4c3b-f4aa-8c8b39b685b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fangningzheng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/fangningzheng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/fangningzheng/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/fangningzheng/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/fangningzheng/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pdfminer.high_level import extract_text # pip install pdfminer.six \n",
    "import docx2txt\n",
    "\n",
    "import subprocess #pip install subprocess.run\n",
    "import datefinder #pip install datefinder\n",
    "from datetime import datetime\n",
    "from find_job_titles import FinderAcora #pip install find-job-titles\n",
    "finder=FinderAcora()\n",
    "\n",
    "from collections import defaultdict #pip install collection\n",
    "import ner #pip install ner\n",
    "import pyap #pip install pyap\n",
    "import pandas as pd\n",
    "\n",
    "from prettytable import PrettyTable #pip install prettytable\n",
    "\n",
    "import nltk #pip install --user -U nltk\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp_model = en_core_web_sm.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKv3kPFiuwWS"
   },
   "source": [
    "# 2. Run the following code sections to load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "6tZk6Y4LuwWT"
   },
   "outputs": [],
   "source": [
    "# extract text from pdf\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    " \n",
    "# extract text from doc\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "JuH3JmeguwWT"
   },
   "outputs": [],
   "source": [
    "# append text in each resume_section\n",
    "def parse_txt_to_section(text):\n",
    "    text_lower = text.lower()\n",
    "    # define modules in resume\n",
    "    module_name = ['experience','project','education','skill']\n",
    "    # initialize index for each section\n",
    "    section_ind = {'experience': -1, 'skill': -1, 'education': -1, 'project': -1}\n",
    "    # initialize text for each section\n",
    "    section_text = {'personal_info': '', 'experience': '', 'skill':'', 'education': ''}\n",
    "    \n",
    "    ind = []\n",
    "    for module in module_name:\n",
    "        # append the index for each section title\n",
    "        if module in list(section_ind.keys()):\n",
    "            section_ind[module] = text_lower.find(module)\n",
    "            #print(module, text.find(module))\n",
    "        ind.append(text_lower.find(module))\n",
    "    \n",
    "    if not ind:\n",
    "        return None\n",
    "    \n",
    "    # sort ind\n",
    "    ind.sort()\n",
    "    \n",
    "    # append personal info from input text\n",
    "    if ind[0] and (ind[0] > 0):\n",
    "        section_text['personal_info'] = text[:ind[0]] \n",
    "    else:\n",
    "        section_text['personal_info'] = text[:ind[1]] \n",
    "    \n",
    "    # append experience from input text\n",
    "    if section_ind['experience'] >= 0:\n",
    "        ind_2 = ind.index(section_ind['experience'])+1 if (ind.index(section_ind['experience'])+1<len(ind)) else -1\n",
    "        section_text['experience'] = text[section_ind['experience']:(ind[ind_2] if (ind_2 != -1) else ind_2)]\n",
    "    \n",
    "        ind_22 = ind.index(section_ind['project'])+1 if (ind.index(section_ind['project'])+1<len(ind)) else -1\n",
    "        section_text['experience'] += text[section_ind['project']:(ind[ind_22] if (ind_22 != -1) else ind_22)]\n",
    "    \n",
    "    \n",
    "    # append skill from input text\n",
    "    if section_ind['skill'] >= 0:\n",
    "        ind_3 = ind.index(section_ind['skill'])+1 if (ind.index(section_ind['skill'])+1<len(ind)) else -1\n",
    "        section_text['skill'] = text[section_ind['skill']:(ind[ind_3] if (ind_3 != -1) else ind_3)]\n",
    "    \n",
    "    # append education from input text\n",
    "    if section_ind['education'] >= 0:\n",
    "        ind_4 = ind.index(section_ind['education'])+1 if (ind.index(section_ind['education'])+1<len(ind)) else -1\n",
    "        section_text['education'] = text[section_ind['education']:(ind[ind_4] if (ind_4 != -1) else ind_4)]\n",
    "    \n",
    "    return section_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "3QnSJ-KOuwWU"
   },
   "outputs": [],
   "source": [
    "def extract_names(text):\n",
    "    \"\"\"person_names = []\n",
    " \n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                person_names.append(\n",
    "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
    "                )\n",
    "                \n",
    "    if not person_names:\"\"\"\n",
    "    temp = text.split()\n",
    "    person_names = [temp[0] + ' ' + temp[1]]\n",
    "    \n",
    "    return person_names[0]\n",
    "    \n",
    "def find_email(text):  \n",
    "    results = re.findall('\\S+@\\S+', text)  \n",
    "    emails = \"\"\n",
    "    for x in results:\n",
    "        emails += str(x)\n",
    "    return emails\n",
    "\n",
    "def find_phone(text): \n",
    "    try:\n",
    "        results = re.findall('((?:\\+\\d{2}[-\\.\\s]??|\\d{4}[-\\.\\s]??)?(?:\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4}))', text)  \n",
    "        x=\"\"\n",
    "        for s in results[0]:\n",
    "            if s.isdigit():\n",
    "                x+=s\n",
    "        phone = int(x)\n",
    "        return phone\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def extract_skills(text, skills_db):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(text)\n",
    " \n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    " \n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    " \n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    " \n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    " \n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in skills_db:\n",
    "            found_skills.add(token)\n",
    " \n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in skills_db:\n",
    "            found_skills.add(ngram)\n",
    " \n",
    "    return found_skills\n",
    "    \n",
    "\n",
    "def extract_education(text):\n",
    "\n",
    "    YEAR = r'(((20|19)(\\d{2})))'\n",
    "    EDUCATION = ['be', 'b.e.', 'b.e', 'bs', 'b.s', 'me', 'm.e', 'm.e.', 'ms', 'm.s', 'btech', 'mtech', \\\n",
    "            'ssc', 'hsc', 'cbse', 'icse', 'x', 'xii', 'phd', 'master', 'bachelor']\n",
    "    \n",
    "    DICT_SCHOOL = [\n",
    "    'school',\n",
    "    'college',\n",
    "    'university',\n",
    "    'academy',\n",
    "    'faculty',\n",
    "    'institute',\n",
    "    'faculdades',\n",
    "    'Schola',\n",
    "    'schule',\n",
    "    'lise',\n",
    "    'lyceum',\n",
    "    'lycee',\n",
    "    'polytechnic',\n",
    "    'kolej',\n",
    "    'Ã¼nivers',\n",
    "    'okul',\n",
    "    ]\n",
    "\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "    out = {'school': [], 'degree': [], 'degree_date':[]}\n",
    "    # Extract school \n",
    "    try:\n",
    "        for i, tex in enumerate(text.split('\\n')):\n",
    "            tex = re.sub(r'[?|$|.|!|,|(|)]', r'', tex)\n",
    "            for j, word in enumerate(tex.split(' ')):\n",
    "                if (word.lower() in DICT_SCHOOL) and (word.lower() not in STOPWORDS) and (tex not in out['school']):\n",
    "                    out['school'].append(\" \".join(tex.split()))\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    # Extract school education degree\n",
    "    try:\n",
    "        for i, tex in enumerate(text.split('\\n')):\n",
    "            tex = re.sub(r'[?|$|.|!|,|(|)]', r'', tex)\n",
    "            for j, word in enumerate(tex.split(' ')):\n",
    "                if word.lower() in EDUCATION and word.lower() not in STOPWORDS:\n",
    "                    out['degree'].append(word)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    # find dates\n",
    "    matches_date = datefinder.find_dates(text)\n",
    "    \n",
    "    \n",
    "    dt2 = list(matches_date)\n",
    "    for date in dt2:\n",
    "        #print(date.strftime('%Y-%m-%d'))\n",
    "        out['degree_date'].append(date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    return out    \n",
    "\n",
    "\n",
    "def extract_experiences(text):\n",
    "    \n",
    "    text = text.replace(\"-\", \"aaa\" )\n",
    "    out = defaultdict(list)\n",
    "    \n",
    "    # match for job title\n",
    "    try:\n",
    "        matches = finder.findall(text)\n",
    "\n",
    "\n",
    "\n",
    "        # process all date data to be used in 'append dates' section\n",
    "        # find all dates\n",
    "        matches_date = list(datefinder.find_dates(text, source=True, index=True))\n",
    "\n",
    "        date_ind=[]\n",
    "        for i, date in enumerate(matches_date):\n",
    "            numm = [s for s in date[1].split() if s.isdigit()]\n",
    "            # append to date_ind with conditions\n",
    "            if numm and (len(numm[0]) >= 4) and (int(numm[0]) >= 1900):\n",
    "                date_ind.append((date[2][0],i))\n",
    "\n",
    "\n",
    "        # append to dict\n",
    "        for i, match in enumerate(matches):\n",
    "            # find front and end index of text\n",
    "            ind_front = max(int(match[0])-100,0)\n",
    "            ind_end = min(int(match[0])+100,len(text))\n",
    "\n",
    "            # append job title\n",
    "            out['experience'+str(i)].append(match[2])\n",
    "\n",
    "            # append company name\n",
    "            ner_list = nlp_model(text[ind_front:ind_end])\n",
    "            potential_comp_name = [X.text for X in ner_list.ents if X.label_=='ORG']\n",
    "            out['experience'+str(i)].append(potential_comp_name)\n",
    "\n",
    "            # append dates\n",
    "            try:\n",
    "                # find the nearest two dates with job title index\n",
    "                ind_near = nearest_n_ind(match[0], date_ind, 2)\n",
    "\n",
    "                # append dates to experience dict\n",
    "                dt1 = matches_date[ind_near[0]][0]\n",
    "                out['experience'+str(i)].append(dt1.strftime('%Y-%m-%d'))\n",
    "\n",
    "                # append second date if the second date is greater than the first date\n",
    "                if matches_date[ind_near[1]][0] >= matches_date[ind_near[0]][0]:\n",
    "                    dt2 = matches_date[ind_near[1]][0]\n",
    "                    out['experience'+str(i)].append(dt2.strftime('%Y-%m-%d'))\n",
    "\n",
    "                else:\n",
    "                    dt2 = datetime.today()\n",
    "                    out['experience'+str(i)].append(dt2.strftime('%Y-%m-%d'))\n",
    "\n",
    "                # append experience duration\n",
    "                date_length = (dt2-dt1)\n",
    "                year_exp = date_length.days/365\n",
    "                out['experience'+str(i)].append(year_exp)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        return out\n",
    "\n",
    "    return out\n",
    "\n",
    "def experience_year(list1):\n",
    "    exp_set = []\n",
    "    # find unique job experience title\n",
    "    for key in list1.keys():\n",
    "        exp_set.append(list1[key][0])\n",
    "    \n",
    "    # append experience's duration\n",
    "    experience_year = defaultdict(list)\n",
    "    try:\n",
    "        for key in list1.keys():\n",
    "            if not experience_year[str(list1[key][0])]:\n",
    "                experience_year['duration: ' + str(list1[key][0])] = 0\n",
    "            if list1[key][0] in set(exp_set):\n",
    "                experience_year['duration: ' + str(list1[key][0])] += (list1[key][4])\n",
    "    except:\n",
    "        return experience_year\n",
    "    \n",
    "    return experience_year\n",
    "    \n",
    "def nearest_n_ind(ind_target, ind_list, n):\n",
    "    minus = abs(np.array(ind_list) - ind_target)\n",
    "    minus_set = [(mi[0],li) for mi, li in zip(minus,ind_list)]\n",
    "    minus_set.sort()\n",
    "    return_set = [ele[1][1] for ele in minus_set]\n",
    "    return return_set[:n]\n",
    "\n",
    "def parse_module(section_text):\n",
    "    # parse personal info\n",
    "    personal_info = {'name': '', 'phone': '', 'email': '', 'address':''}\n",
    "    personal_info['name'] = extract_names(section_text['personal_info'])\n",
    "    personal_info['email'] = find_email(section_text['personal_info'])\n",
    "    personal_info['phone'] = find_phone(section_text['personal_info'])\n",
    "    personal_info['address'] = re.findall(\"[0-9]{1,3} .+ .+ [A-Za-z]{2} [0-9]{5}\", section_text['personal_info'])\n",
    "    \n",
    "    \n",
    "    # parse education\n",
    "    education = extract_education(section_text['education'])\n",
    "    \n",
    "    # parse experience\n",
    "    experience = extract_experiences(section_text['experience'])\n",
    "    if experience:\n",
    "        experience_duration = experience_year(experience)\n",
    "    else:\n",
    "        experience_duration = defaultdict(list)\n",
    "    \n",
    "    # parse skills\n",
    "    data = pd.read_csv('./skill_12_12_2022_Master_v5.csv')\n",
    "    skills_db = list(data.iloc[:, 1])\n",
    "    skill = extract_skills(section_text['experience'], skills_db)\n",
    "    skill.union(extract_skills(section_text['skill'], skills_db))\n",
    "    skill_dict = {'skill':list(skill)}\n",
    "    \n",
    "    # merge dictionaries\n",
    "    resume_section = {**personal_info, **education, **experience, **experience_duration, **skill_dict}\n",
    "    \n",
    "    return resume_section\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBRZCQYEuwWV"
   },
   "source": [
    "# 3. Run the main code\n",
    "### 3.1 define the path for 'path_main_dir' (working dir) and 'path_to_doc' (path to resumes to be tested)\n",
    "### 3.2 the 'resume_info_all' dictionary will store all resume info parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "S3_QAiyKuwWW"
   },
   "outputs": [],
   "source": [
    "# initial the big dictionary\n",
    "resume_info_all = defaultdict(list)\n",
    "\n",
    "# get filenames\n",
    "path_main_dir = \"your path to resumes"\n",
    "path_to_doc = path_main_dir + \"/resume_examples\"\n",
    "os.chdir(path_to_doc)\n",
    "files = [f for f in os.listdir('.') if os.path.isfile(f) and (f.endswith('.docx') or f.endswith('.pdf'))]\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    # convert doc/pdf to txt\n",
    "    os.chdir(path_to_doc)\n",
    "    if (file[0] not in '~') and file.endswith('.docx'):\n",
    "        text = extract_text_from_docx(file)\n",
    "\n",
    "    elif (file[0] not in '~') and file.endswith('.pdf'):\n",
    "        text = extract_text_from_pdf(file)\n",
    "\n",
    "        \n",
    "    os.chdir(path_main_dir)\n",
    "    \n",
    "    # get section text\n",
    "    section_text = parse_txt_to_section(text)\n",
    "    \n",
    "    # get section info into dict\n",
    "    resume_section = parse_module(section_text)\n",
    "\n",
    "    \n",
    "    for key, value in resume_section.items():\n",
    "        # append value to the correct index\n",
    "        if key not in resume_info_all.keys():\n",
    "            resume_info_all[key] = [None] * i\n",
    "            resume_info_all[key].insert(i, value)\n",
    "            \n",
    "        else:\n",
    "            resume_info_all[key].insert(i, value)\n",
    "            \n",
    "    for key in resume_info_all.keys():\n",
    "        if not key in resume_section.keys():\n",
    "            resume_info_all[key].insert(i, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHMN8wbjuwWX"
   },
   "outputs": [],
   "source": [
    "print(resume_info_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvXq3NMAuwWX"
   },
   "source": [
    "# 4. Visualize resume information in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "5IBlQM8juwWY",
    "outputId": "9da23109-4359-4424-be5c-ed9082371f2e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>phone</th>\n",
       "      <th>email</th>\n",
       "      <th>address</th>\n",
       "      <th>school</th>\n",
       "      <th>degree</th>\n",
       "      <th>degree_date</th>\n",
       "      <th>experience0</th>\n",
       "      <th>experience1</th>\n",
       "      <th>experience2</th>\n",
       "      <th>...</th>\n",
       "      <th>Systems Analyst</th>\n",
       "      <th>duration: Systems Analyst</th>\n",
       "      <th>Accountant</th>\n",
       "      <th>duration: Accountant</th>\n",
       "      <th>Project Director</th>\n",
       "      <th>duration: Project Director</th>\n",
       "      <th>Research Assistant</th>\n",
       "      <th>duration: Research Assistant</th>\n",
       "      <th>Product Development</th>\n",
       "      <th>duration: Product Development</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BINGXING CHEN</td>\n",
       "      <td>2178982099</td>\n",
       "      <td>estelabx1122@gmail.com</td>\n",
       "      <td>[]</td>\n",
       "      <td>[University of Illinois at Urbana-Champaign Ur...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[2021-05-30]</td>\n",
       "      <td>[Consulting Intern, [IL\\n\\n Consulting Intern]...</td>\n",
       "      <td>[Research Intern, [l manufactory company, Ifly...</td>\n",
       "      <td>[Marketing Analyst, [Anhui Dafu Group], 2019-0...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aqua Wan</td>\n",
       "      <td>2027659700</td>\n",
       "      <td>aquawan1109@gmail.com</td>\n",
       "      <td>[]</td>\n",
       "      <td>[American University -Kogod School of Business...</td>\n",
       "      <td>[Bachelor]</td>\n",
       "      <td>[2020-05-30]</td>\n",
       "      <td>[Media Operator, [Social Media, D.C. Branch   ...</td>\n",
       "      <td>[Social Media Assistant, [Social Media], 2017-...</td>\n",
       "      <td>[Sales Representatives, [], 2014-08-30, 2023-0...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baiyu (Novak)</td>\n",
       "      <td>7185010806</td>\n",
       "      <td>novakliubaiyu@gmail.com</td>\n",
       "      <td>[]</td>\n",
       "      <td>[New York University New York NY, St Johnâs Un...</td>\n",
       "      <td>[Master, Bachelor]</td>\n",
       "      <td>[2019-05-30, 2017-05-30]</td>\n",
       "      <td>[Partner, [], 2019-07-30, 2019-08-30, 0.084931...</td>\n",
       "      <td>[Risk Intern, [NY\\n\\n     Enterprise], 2019-07...</td>\n",
       "      <td>[Risk Intern, [], 2018-07-30, 2018-08-30, 0.08...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Claire Zhang</td>\n",
       "      <td>7783217997</td>\n",
       "      <td>clairezhangbb@gmail.com</td>\n",
       "      <td>[]</td>\n",
       "      <td>[University of British Columbia Sauder School ...</td>\n",
       "      <td>[Bachelor]</td>\n",
       "      <td>[2019-12-30]</td>\n",
       "      <td>[Operations Intern, [], 2014-04-30, 2015-04-30...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Binhe (Betty)</td>\n",
       "      <td>6464219329</td>\n",
       "      <td>bx2167@columbia.edu</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Columbia University - Columbia Engineering Ne...</td>\n",
       "      <td>[MS]</td>\n",
       "      <td>[2020-12-30, 2017-12-30]</td>\n",
       "      <td>[Partner, [Allied Millennial Partners, LLC New...</td>\n",
       "      <td>[Financial Analyst Intern, [China Internationa...</td>\n",
       "      <td>[Research Intern, [], 2016-07-30, 2018-07-30, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bei Zhang</td>\n",
       "      <td>7816700695</td>\n",
       "      <td>bzhang_1900@berkeley.edu</td>\n",
       "      <td>[122 University Ave Apt 513| Berkeley, CA 94702]</td>\n",
       "      <td>[University of California Berkeley Expected Ma...</td>\n",
       "      <td>[Bachelor]</td>\n",
       "      <td>[2021-05-30]</td>\n",
       "      <td>[VP, [n a proper international control, SkyDec...</td>\n",
       "      <td>[Team Member, [], 2018-01-30, 2023-01-30, 5.00...</td>\n",
       "      <td>[Consulting Intern, [], 2018-01-30, 2023-01-30...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WANGYINJUN (ADAM)</td>\n",
       "      <td>7654795757</td>\n",
       "      <td>|wangyinjun.song@columbia.edu</td>\n",
       "      <td>[450 W 42nd St. Apt. 41A, New York NY 10036]</td>\n",
       "      <td>[Columbia University GPA: 350/4 New York NY, P...</td>\n",
       "      <td>[BS]</td>\n",
       "      <td>[2018-12-30, 2014-09-30, 2017-05-30, 2023-03-30]</td>\n",
       "      <td>[Investment Analyst, [], 2018-06-30, 2018-08-3...</td>\n",
       "      <td>[Research Analyst, [NY\\n\\nResearch Analyst, He...</td>\n",
       "      <td>[Partner, [Independence Capital Asset Partners...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Baiyu (Novak)</td>\n",
       "      <td>7185010806</td>\n",
       "      <td>novakliubaiyu@gmail.com</td>\n",
       "      <td>[]</td>\n",
       "      <td>[New York University New York NY, St Johnâs Un...</td>\n",
       "      <td>[Master, Bachelor]</td>\n",
       "      <td>[2019-05-30, 2017-05-30]</td>\n",
       "      <td>[Risk Intern, [], 2018-07-30, 2018-08-30, 0.08...</td>\n",
       "      <td>[Risk Intern, [], 2018-05-30, 2018-07-30, 0.16...</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Layla Martin</td>\n",
       "      <td>5202712492</td>\n",
       "      <td>layla.d.martin@gmail.com</td>\n",
       "      <td>[]</td>\n",
       "      <td>[University of San Francisco San Francisco CA,...</td>\n",
       "      <td>[Master, Bachelor]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MONICA MEYER</td>\n",
       "      <td></td>\n",
       "      <td>monica.meyer@comcast.net</td>\n",
       "      <td>[]</td>\n",
       "      <td>[University of San Francisco, University of Ca...</td>\n",
       "      <td>[Master, Bachelor]</td>\n",
       "      <td>[2023-01-03]</td>\n",
       "      <td>[Sales and Service Specialist, [Bank of Americ...</td>\n",
       "      <td>[Sales and Service Specialist, [Service Specia...</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Brendan Herger</td>\n",
       "      <td>4155827457</td>\n",
       "      <td>13herger@gmail.com</td>\n",
       "      <td>[209 Page Street No. 7 San Francisco, Ca 94117]</td>\n",
       "      <td>[University of San Francisco July 2015, Univer...</td>\n",
       "      <td>[MS, BS]</td>\n",
       "      <td>[2015-07-30, 2014-05-30]</td>\n",
       "      <td>[Data Scientist, []]</td>\n",
       "      <td>[Supervisor, [ill Architecture (, Ca.) &amp;, Nati...</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>John Smith</td>\n",
       "      <td>6145555555</td>\n",
       "      <td>sresume@kent.edu</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Kent State University]</td>\n",
       "      <td>[Bachelor]</td>\n",
       "      <td>[2023-01-03]</td>\n",
       "      <td>[Resident Advisor, [], 2015-08-30, 2023-01-30,...</td>\n",
       "      <td>[Assistant Coach, [], 2015-05-30, 2023-01-30, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Jing (Abby)</td>\n",
       "      <td>9185689736</td>\n",
       "      <td>abby.li0824@gmail.com</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Oklahoma City University Oklahoma City OK, Be...</td>\n",
       "      <td>[Master, Bachelor]</td>\n",
       "      <td>[2013-05-30, 2009-06-30]</td>\n",
       "      <td>[Systems Analyst, [], 2018-10-30, 2023-01-30, ...</td>\n",
       "      <td>[Accountant, [], 2014-09-30, 2018-09-30, 4.002...</td>\n",
       "      <td>[Accountant, [], 2013-08-30, 2018-09-30, 5.087...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>4.254795</td>\n",
       "      <td>[]</td>\n",
       "      <td>5.087671</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SÃ©bastien Genty</td>\n",
       "      <td>7133015648</td>\n",
       "      <td>sgenty@me.com</td>\n",
       "      <td>[209 Page St, Apt 7, San Francisco, CA 94117]</td>\n",
       "      <td>[Bucknell University â Lewisburg PA]</td>\n",
       "      <td>[BS]</td>\n",
       "      <td>[2012-05-30]</td>\n",
       "      <td>[Project Director, [Socratic Technologies], 20...</td>\n",
       "      <td>[Research Assistant, [Bucknell University], 20...</td>\n",
       "      <td>[Research Intern, [MD Anderson Cancer Center],...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>10.339726</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows Ã 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name       phone                          email  \\\n",
       "0       BINGXING CHEN  2178982099         estelabx1122@gmail.com   \n",
       "1            Aqua Wan  2027659700          aquawan1109@gmail.com   \n",
       "2       Baiyu (Novak)  7185010806        novakliubaiyu@gmail.com   \n",
       "3        Claire Zhang  7783217997        clairezhangbb@gmail.com   \n",
       "4       Binhe (Betty)  6464219329            bx2167@columbia.edu   \n",
       "5           Bei Zhang  7816700695       bzhang_1900@berkeley.edu   \n",
       "6   WANGYINJUN (ADAM)  7654795757  |wangyinjun.song@columbia.edu   \n",
       "7       Baiyu (Novak)  7185010806        novakliubaiyu@gmail.com   \n",
       "8        Layla Martin  5202712492       layla.d.martin@gmail.com   \n",
       "9        MONICA MEYER                   monica.meyer@comcast.net   \n",
       "10     Brendan Herger  4155827457             13herger@gmail.com   \n",
       "11         John Smith  6145555555               sresume@kent.edu   \n",
       "12        Jing (Abby)  9185689736          abby.li0824@gmail.com   \n",
       "13    SÃ©bastien Genty  7133015648                  sgenty@me.com   \n",
       "\n",
       "                                             address  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "5   [122 University Ave Apt 513| Berkeley, CA 94702]   \n",
       "6       [450 W 42nd St. Apt. 41A, New York NY 10036]   \n",
       "7                                                 []   \n",
       "8                                                 []   \n",
       "9                                                 []   \n",
       "10   [209 Page Street No. 7 San Francisco, Ca 94117]   \n",
       "11                                                []   \n",
       "12                                                []   \n",
       "13     [209 Page St, Apt 7, San Francisco, CA 94117]   \n",
       "\n",
       "                                               school              degree  \\\n",
       "0   [University of Illinois at Urbana-Champaign Ur...                  []   \n",
       "1   [American University -Kogod School of Business...          [Bachelor]   \n",
       "2   [New York University New York NY, St Johnâs Un...  [Master, Bachelor]   \n",
       "3   [University of British Columbia Sauder School ...          [Bachelor]   \n",
       "4   [Columbia University - Columbia Engineering Ne...                [MS]   \n",
       "5   [University of California Berkeley Expected Ma...          [Bachelor]   \n",
       "6   [Columbia University GPA: 350/4 New York NY, P...                [BS]   \n",
       "7   [New York University New York NY, St Johnâs Un...  [Master, Bachelor]   \n",
       "8   [University of San Francisco San Francisco CA,...  [Master, Bachelor]   \n",
       "9   [University of San Francisco, University of Ca...  [Master, Bachelor]   \n",
       "10  [University of San Francisco July 2015, Univer...            [MS, BS]   \n",
       "11                            [Kent State University]          [Bachelor]   \n",
       "12  [Oklahoma City University Oklahoma City OK, Be...  [Master, Bachelor]   \n",
       "13               [Bucknell University â Lewisburg PA]                [BS]   \n",
       "\n",
       "                                         degree_date  \\\n",
       "0                                       [2021-05-30]   \n",
       "1                                       [2020-05-30]   \n",
       "2                           [2019-05-30, 2017-05-30]   \n",
       "3                                       [2019-12-30]   \n",
       "4                           [2020-12-30, 2017-12-30]   \n",
       "5                                       [2021-05-30]   \n",
       "6   [2018-12-30, 2014-09-30, 2017-05-30, 2023-03-30]   \n",
       "7                           [2019-05-30, 2017-05-30]   \n",
       "8                                                 []   \n",
       "9                                       [2023-01-03]   \n",
       "10                          [2015-07-30, 2014-05-30]   \n",
       "11                                      [2023-01-03]   \n",
       "12                          [2013-05-30, 2009-06-30]   \n",
       "13                                      [2012-05-30]   \n",
       "\n",
       "                                          experience0  \\\n",
       "0   [Consulting Intern, [IL\\n\\n Consulting Intern]...   \n",
       "1   [Media Operator, [Social Media, D.C. Branch   ...   \n",
       "2   [Partner, [], 2019-07-30, 2019-08-30, 0.084931...   \n",
       "3   [Operations Intern, [], 2014-04-30, 2015-04-30...   \n",
       "4   [Partner, [Allied Millennial Partners, LLC New...   \n",
       "5   [VP, [n a proper international control, SkyDec...   \n",
       "6   [Investment Analyst, [], 2018-06-30, 2018-08-3...   \n",
       "7   [Risk Intern, [], 2018-07-30, 2018-08-30, 0.08...   \n",
       "8                                                None   \n",
       "9   [Sales and Service Specialist, [Bank of Americ...   \n",
       "10                               [Data Scientist, []]   \n",
       "11  [Resident Advisor, [], 2015-08-30, 2023-01-30,...   \n",
       "12  [Systems Analyst, [], 2018-10-30, 2023-01-30, ...   \n",
       "13  [Project Director, [Socratic Technologies], 20...   \n",
       "\n",
       "                                          experience1  \\\n",
       "0   [Research Intern, [l manufactory company, Ifly...   \n",
       "1   [Social Media Assistant, [Social Media], 2017-...   \n",
       "2   [Risk Intern, [NY\\n\\n     Enterprise], 2019-07...   \n",
       "3                                                None   \n",
       "4   [Financial Analyst Intern, [China Internationa...   \n",
       "5   [Team Member, [], 2018-01-30, 2023-01-30, 5.00...   \n",
       "6   [Research Analyst, [NY\\n\\nResearch Analyst, He...   \n",
       "7   [Risk Intern, [], 2018-05-30, 2018-07-30, 0.16...   \n",
       "8                                                None   \n",
       "9   [Sales and Service Specialist, [Service Specia...   \n",
       "10  [Supervisor, [ill Architecture (, Ca.) &, Nati...   \n",
       "11  [Assistant Coach, [], 2015-05-30, 2023-01-30, ...   \n",
       "12  [Accountant, [], 2014-09-30, 2018-09-30, 4.002...   \n",
       "13  [Research Assistant, [Bucknell University], 20...   \n",
       "\n",
       "                                          experience2  ... Systems Analyst  \\\n",
       "0   [Marketing Analyst, [Anhui Dafu Group], 2019-0...  ...            None   \n",
       "1   [Sales Representatives, [], 2014-08-30, 2023-0...  ...            None   \n",
       "2   [Risk Intern, [], 2018-07-30, 2018-08-30, 0.08...  ...            None   \n",
       "3                                                None  ...            None   \n",
       "4   [Research Intern, [], 2016-07-30, 2018-07-30, ...  ...            None   \n",
       "5   [Consulting Intern, [], 2018-01-30, 2023-01-30...  ...            None   \n",
       "6   [Partner, [Independence Capital Asset Partners...  ...            None   \n",
       "7                                                None  ...            None   \n",
       "8                                                None  ...            None   \n",
       "9                                                None  ...            None   \n",
       "10                                               None  ...            None   \n",
       "11                                               None  ...            None   \n",
       "12  [Accountant, [], 2013-08-30, 2018-09-30, 5.087...  ...              []   \n",
       "13  [Research Intern, [MD Anderson Cancer Center],...  ...            None   \n",
       "\n",
       "   duration: Systems Analyst Accountant duration: Accountant  \\\n",
       "0                        NaN       None                  NaN   \n",
       "1                        NaN       None                  NaN   \n",
       "2                        NaN       None                  NaN   \n",
       "3                        NaN       None                  NaN   \n",
       "4                        NaN       None                  NaN   \n",
       "5                        NaN       None                  NaN   \n",
       "6                        NaN       None                  NaN   \n",
       "7                        NaN       None                  NaN   \n",
       "8                        NaN       None                  NaN   \n",
       "9                        NaN       None                  NaN   \n",
       "10                       NaN       None                  NaN   \n",
       "11                       NaN       None                  NaN   \n",
       "12                  4.254795         []             5.087671   \n",
       "13                       NaN       None                  NaN   \n",
       "\n",
       "    Project Director duration: Project Director  Research Assistant  \\\n",
       "0               None                        NaN                None   \n",
       "1               None                        NaN                None   \n",
       "2               None                        NaN                None   \n",
       "3               None                        NaN                None   \n",
       "4               None                        NaN                None   \n",
       "5               None                        NaN                None   \n",
       "6               None                        NaN                None   \n",
       "7               None                        NaN                None   \n",
       "8               None                        NaN                None   \n",
       "9               None                        NaN                None   \n",
       "10              None                        NaN                None   \n",
       "11              None                        NaN                None   \n",
       "12              None                        NaN                None   \n",
       "13                []                  10.339726                  []   \n",
       "\n",
       "   duration: Research Assistant  Product Development  \\\n",
       "0                           NaN                 None   \n",
       "1                           NaN                 None   \n",
       "2                           NaN                 None   \n",
       "3                           NaN                 None   \n",
       "4                           NaN                 None   \n",
       "5                           NaN                 None   \n",
       "6                           NaN                 None   \n",
       "7                           NaN                 None   \n",
       "8                           NaN                 None   \n",
       "9                           NaN                 None   \n",
       "10                          NaN                 None   \n",
       "11                          NaN                 None   \n",
       "12                          NaN                 None   \n",
       "13                          1.0                   []   \n",
       "\n",
       "   duration: Product Development  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4                            NaN  \n",
       "5                            NaN  \n",
       "6                            NaN  \n",
       "7                            NaN  \n",
       "8                            NaN  \n",
       "9                            NaN  \n",
       "10                           NaN  \n",
       "11                           NaN  \n",
       "12                           NaN  \n",
       "13                           1.0  \n",
       "\n",
       "[14 rows x 68 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the all_skills dictionary\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in resume_info_all.items() ]))\n",
    "pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in resume_info_all.items() ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAr-RVnluwWY"
   },
   "source": [
    "# 5. Save the dictionary into .csv or .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Vnq9nmbhuwWZ"
   },
   "outputs": [],
   "source": [
    "# save dict to csv file\n",
    "df.to_csv('out_RP_by_FZ.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0odmTASGuwWZ"
   },
   "outputs": [],
   "source": [
    "# save to json file\n",
    "with open(\"resumeparser_out_FZ.json\", \"w\") as outfile:\n",
    "    json.dump(resume_info_all, outfile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
